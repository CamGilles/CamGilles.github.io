<!DOCTYPE html> 
<html lang="fr">
    <head> 
        <meta charset="UTF-8"> 
        <title>un site</title> 
        </head> 
        <link rel="stylesheet" href="ex2.css"/>
        <body>
            <div>
                <img id="page1" src="Page.png"/>

                <h1>
                    Rapport : Etude et perspectives de la conférence sur l’intelligence artificielle et la vie privée
                </h1>
                <p>DUMONT TIPHAINE, HERBOCH BENJAMIN, GILLES CAMILLE, DUPIRE LUCIE</p>
                <p>
                    Depuis 2017, la CNIL appelle à la vigilance concernant les évolutions des outils de vidéoprotection. En effet, la
multiplication de certains dispositifs de vidéo « augmentée » pose des questions éthiques et juridiques nouvelles.
Aujourd’hui, afin d’accompagner leur déploiement dans le respect des droits des personnes, la CNIL soumet un projet
de proposition à consultation publique jusqu’au 11 mars 2022. De tels dispositifs ne sont en aucun cas un simple
« prolongement » technique des caméras existantes. Ils modifient leur nature même par leur capacité de détection et
d’analyse automatisée.
                </p><br>
                <p>L’intelligence artificielle (IA) désigne tout système capable d’accomplir des tâches d’une manière que nous
                    percevons comme “intelligente”. Autrement dit, il s’agit de l’ensemble de techniques, théories et sciences qui
                    simulent les capacités cognitives de l’être humain1. Ainsi, l’abstraction, la créativité, la déduction, la résolution de
                    problèmes, la prise de décision ou la capacité d’apprendre, sont associées à l’IA. Une différence doit notamment être
                    faite entre IA dite “forte” et IA dite « faible ». L’IA « forte » serait en capacité de contextualiser des problèmes
                    spécialisés très différents de manière totalement autonome.</p><br>
                    <p>Toutefois, selon le Conseil de l’Europe, aucune technologie connue ne permet de démontrer l’existence actuelle
                        d’IA « forte ». Par opposition, l’IA « faible » ou “modérée” n’est en capacité d’être performante que dans son
                        domaine d'entraînement. Quant au Machine Learning, il représente un ensemble de méthodes puissantes qui
                        permettent de créer des modèles prédictifs à partir de données sans avoir été explicitement programmés. Ce type
                        d’intelligence artificielle est mis en cause aujourd’hui</p><br>
                        <p>Se pose alors la question des conséquences d’une généralisation hâtive de l’Intelligence artificielle. Le chercheur
                            postdoctoral Pégny Maël2, lors de deux matinées d’études organisées par l’Université Paris-Est Créteil, a évoqué les
                            problématiques pouvant exister entre droit au respect de la vie privée et développement des IAs. En effet, le
                            développement de l’Intelligence artificielle a apporté de nombreuses incertitudes quant à son développement éthique
                            en raison du renforcement des inégalités et de l’atteinte à la vie privée que pourrait engendrer son emploi dans
                            différents secteurs tels que notamment la santé, l’assurance ou les services publiques.</p><br>
                            <p>Face aux différents enjeux exposés et à la nécessité d’identifier les différents enjeux et ordres juridiques présents
                                en la matière, il est aujourd’hui essentiel de traiter de la notion d’IA et de vie privée.</p><br>

                                <p>Dans ce cadre, une exposition liminaire du point de vue de Maël Pégny (I) paraît essentielle afin de fixer le cadre
                                    de réflexion. Ainsi, ce cadre permettra dans un second temps d’identifier la nécessité d’une analyse poussée face à
                                    l’ère du développement de l’IA (II) afin de les envisager à l’aune des propositions actuelles (III).</p>
                                    <h1>I. Une appréhension globalisée du droit qu respect de la vie privée et développement des IAs</h1><br>
                                    <p>Au sein de sa restitution et après avoir exposé le fait que la vie privée est une donnée entretenant des liens
                                        importants avec les institutions étatiques, Maël Pégny évoque les enjeux de la collecte de données personnelles, à
                                        savoir les questions relatives à la collecte et la formation de connaissances (réelles ou prétendues) sur les individus.
                                        L’explosion de la collecte induit l’automatisation de l’analyse. On définit l’intelligence artificielle par son objectif :
                                        créer des systèmes informatiques intelligents en exécutant des tâches communément considérées comme
                                        intelligentes.</p><br>
                                        <p>Une fois ces définitions posées, Maël Pégny présente les problématiques relatives à « L’âge d’or de la
                                            surveillance3” : la surveillance de masse en passant par le capitalisme de la surveillance pour finir par la propagande
                                            de masse et le profilage des individus. Enfin ont été abordées les notions de mort de l’anonymat en ligne et les enjeux
                                            qu’elle soulève</p><br>
                                            <p>S’appliquent à l’IA des prises de position fortes pour interdire certaines pratiques économiques manipulatoires,
                                                exploitatrices ou visant au contrôle social. L’IA fait également face à une exigence de transparence, en informant sur
                                                l’existence d’une interaction avec un robot ou sur la détection d’émotion. Maël Pégny soulève la question suivante :
                                                pourquoi ces restrictions s'appliquent- elles seulement à des systèmes qualifiés d’IA ?</p><br>
                                                <p>La reconnaissance faciale est l’exemple type d’intelligence artificielle. Elle regroupe deux formes d’outils : la
                                                    reconnaissance à partir de photographies et la reconnaissance en direct à partir d’images de caméras de vidéo de surveillance dans l’espace public (FRT). C’est cette seconde forme qui focalise le débat. En effet, elle marque une
                                                    rupture juridique en appliquant à l’ensemble de la population des pratiques d’identification réservées aux suspects et
                                                    criminels avérés, et une rupture symbolique : c’est la fin de l’anonymat dans l’espace public. Ainsi, il ne faut pas
                                                    surestimer l’efficacité de la FRT, qui connaît énormément de dysfonctionnements avec un taux de réussite de 0% sur
                                                    des tests menés en 2016. Cependant, cela pose un problème de fond majeur : que se passerait-il si la technique
                                                    fonctionnait bien ? Deux écoles de pensée s’opposent. D’une part, l’école réglementaire qui défend un usage très
                                                    limité de la reconnaissance faciale dans les cas de terrorisme ou de disparition d’enfant par exemple. D’autre part,
                                                    l’école prohibitionniste qui défend le fait qu’un tel dispositif ne devrait pas exister. En outre, le développement de ce
                                                    type d’IA pose deux questions essentielles : la question du développement de la technologie et notamment des bases
                                                    de données d’entraînement qui doivent être créées, et la question de l’usage de cette technologie et surtout du contrôle
                                                    institutionnel.</p><br>
                                                    <p>On se rend compte de la nécessité de la création de vastes bases de données collectées dans des conditions
                                                        naturelles pour l'entraînement des modèles. L’approche d’usage réglementé entérine la collecte de données à l’échelle
                                                        d’une population sans consentement. Maël Pégny alerte quant à la perte de sensibilité contemporaine face à cette
                                                        collecte et se pose la question de la fidélité à l’esprit originel du droit des données personnelles.</p><br>
                                                        <p>Le développement de la FRT constitue en soi un tournant juridique et le problème du contrôle institutionnel est
                                                            une question cruciale. Est-on capable de borner l’emploi d’un outil de contrôle social ? La question peut
                                                            raisonnablement être soulevée au regard de certaines pratiques inquiétantes dans les polices de certains pays
                                                            démocratiques, notamment aux Etats-Unis qui sont source d’inquiétude comme les identifications de manifestants
                                                            “Black Lives Matter'', ou le programme FBI “COINTELPRO” dont l’objectif fut d'enquêter sur les organisations
                                                            politiques dissidentes aux Etats-Unis et de perturber leurs activités. Dans la Résolution du Parlement européen du 20
                                                            octobre 2020 concernant un cadre pour les aspects éthiques de l’intelligence artificielle, de la robotique et des
                                                            technologies connexes (2020/2012(INL)), est interdite l’identification biométrique à distance dans l’espace public en
                                                            temps réel à des fins répressives. Cependant, au regard du nombre d’exceptions, cela ressemble davantage à une
                                                            autorisation plutôt qu'à une prohibition puisque sont listées toutes les infractions de droit commun</p><br>
                                                            <br>
                                                            <p>Toutes ces questions ont un impact conceptuel sur notre ordre légal et notamment sur les données personnelles.
                                                                C’est une notion extrêmement large et sémantique. L’IA a pour conséquence de déduire des choses des données
                                                                qu’on lui transmet : “Vous ne pouvez protéger ce que je peux déduire”. Ce n’est pas la donnée qu’il faut protéger,
                                                                mais la connaissance qu’elle contient. Dans ce cadre, Maël Pégny alerte sur la nécessité d’une régulation de
                                                                l’inférence statistique4 et notamment sur la validité scientifique des inférences (droit à l’inférence raisonnable) et de
                                                                la protection de la vie privée contre le contournement statistique. Aujourd’hui, il est possible de procéder à des
                                                                inférences d’appartenance (déduction de l’appartenance d’un sujet à la base d’entrainement).</p><br>
                                                                <p>Cependant, le problème prépondérant est que le droit des données de l’UE est centré sur les données avant
                                                                    traitement. Une donnée est personnelle si elle permet d’identifier une personne physique. Il y a une nécessité de
                                                                    clarifier le rapport entre traitement et statut de donnée personnelle. Ces problématiques sont mises en lumière par le
                                                                    Groupe de travail “article 29”5, sur la protection des données. Trois critères des données personnelles peuvent être
                                                                    retenus : contenu, intention, résultat. Avec l’exploitation des données environnementales pour influencer les
                                                                    personnes dans les smart cities6, tout peut devenir une donnée personnelle par intention ou par résultat.</p><br>
                                                                    <p>nfin, Maël Pégny nous rappelle qu’il ne faut pas penser qu’en termes de barrières mais il s’agit plutôt de se poser
                                                                        la question suivante : comment réorienter l’appareil de connaissance numérique des individus vers des fins saines ?
                                                                        Quelle reconnaissance par les institutions les individus souhaitent-ils ?</p><br>
                                                                        <p>En effet, la reconnaissance des individus par les institutions peut être désirée. Ce fut le cas notamment des femmes
                                                                            amérindiennes qui habitaient sur les réserves. Une base centralisée est la première chose qui a été demandée pour
                                                                            leur permettre de jouir des mêmes droits que les femmes américaines, notamment en ce qui concerne les violences
                                                                            sexuelles</p>

                                                                            <h2>II.La nécessité d'une analyse poussée face à l'ère du développement</h2>

                                                                            <h3>A.Une prise en compte de l'éthique dès la conception</h3><br>
                                                                            <h4>1. La nécessité de penser éthiquement l'usage de données personnelles dès la conception</h4><br>
                                                                            <p>Pour Maël Pégny, l’éthique s’envisage en amont de la conception notamment de l’IA pour y inclure les possibles
                                                                                dangers politiques qu’elle pourrait représenter de par la grande collection des données et leur centralisation ainsi que
                                                                                leur exploitation, qui est le point le plus problématique pour la vie privée. Il faut en outre prendre la mesure de ce
                                                                                qu’est une donnée au travers de son accès, sa qualité et notamment son intégrité.</p><br>
                                                                                <p>Force est de constater cependant que la résolution du problème éthique n’en est qu’à ses balbutiements. En
                                                                                    témoigne le rapport de la commission Ethique du Ministère fédéral allemand des transports sur les véhicules
                                                                                    autonomes et connectés, qui met en exergue l’impossibilité pour l’éthique et le juridique d’être inclus directement
                                                                                    dans la programmation, qu’il faut cependant une approche de développement éthique qui parcourt l’ensemble du
                                                                                    cycle de vie du logiciel afin d’en améliorer la conception.</p><br>
                                                                                    <p>Cette idée pose cependant plusieurs problèmes notamment dans l’idée du développement éthique par essence,
                                                                                        puisque les développeur.euses devront obtenir une expertise économique, juridique, sociologique et philosophique
                                                                                        ce qui est une attente irréaliste pour Maël Pégny. Une acception plus réaliste serait une délimitation des difficultés
                                                                                        de l’opérationnalisation, c'est-à-dire l’absence de limites à la création de l’IA qui est stimulée par la concurrence et
                                                                                        le sensationnalisme. Cependant, le numérique et notamment l’IA ne peut plus être pensé uniquement sous un aspect
                                                                                        technique. De par l’usage dont le numérique fait l’objet, il est nécessaire d’envisager l’éthique dès la conception du
                                                                                        service ou du produit. L’intervenant Maël Pégny avait proposé une charte afin de s’assurer que le produit ou le service
                                                                                        était éthique depuis sa conception.</p><br>

                                                                                        <ul>En effet, il évoque dans son écrit Pour un développement des IAs respectueux de la vie privée dès la conception7
                                                                                            le fait de :
                                                                                            <li>déclarer la finalité de l’usage des données en documentant et justifiant tout écart à la déclaration initiale
                                                                                                <li>tester les performances finales du logiciel et limiter le pouvoir prédictif du modèle à ce qui est nécessaire,</li>
                                                                                                <li>mettre en balance risque de suroptimisation et de perte de performances en prenant en compte la vie
                                                                                                    privée,</li>
                                                                                                    <li>entraîner si possible son modèle sans avoir recours aux données personnelles, ou d’utiliser des données
                                                                                                        ayant fait l’objet d’un geste explicite de publication, la mise à jour des données, retrait de publication et
                                                                                                        l’exercice des droits de rectification, d’effacement et d’opposition,</li>
                                                                                                        <li>déclarer les raisons justifiant l’utilisation des données personnelles ainsi que les mesures prises pour lutter
                                                                                                            contre la rétro-ingénierie des données, et prendre position sur leur complétude par rapport aux boîtes à
                                                                                                            outils et aux méthodes d’attaque existantes,</li>
                                                                                                            <li>diffuser en licence libre tous les outils de sécurisation contre la rétro-ingénierie des données,</li>
                                                                                                            <li>mettre le modèle à disposition de tous afin de permettre la vérification publique des propriétés de sécurité
                                                                                                                si cela n'entraîne pas de faille de sécurité intolérable et le justifier,</li>
                                                                                                                <li>lorsque les mesures de restrictions de la collecte et de lutte contre la rétroingénierie ne sont pas applicables
                                                                                                                    et que la gravité de l’enjeu surpasse les enjeux de vie privée,</li>
                                                                                                                    <li>autoriser un modèle encodant des données privées en restreignant strictement l’accès à ce modèle et son
                                                                                                                        emploi pour l’usage ayant justifié l’exception</li>
                                                                                                                    </ul> <br>
                                                                                                                        <p>Cependant, en l’état actuel du droit, la législation a apporté un lot non négligeable de sécurisation et d’éthique du
                                                                                                                            traitement des données qui constitue un commencement à cet idéal de protection qui se dessine au travers de la charte
                                                                                                                            telle que conçue par Maël Pégny</p>

                                                                                                                            <h4>2. L'état du droit</h4>

                                                                                                                            <h4>3. L'objet d'une charte développement éthique</h4>

                                                                                                                            <h3>B.L'éthique de la surveillance</h3>

                                                                                                                            <h4>1. La surveillance étatique</h4>
                                                                                                                            <h4>2. Les algorithmes discrminatoires</h4>

                                                                                                                            <h3>C. Les approches juridiques et philosophiques</h3>
                                                                                                                            <h4>L'inapplicabilité de la dichotomie données logiciels, en matière de modèles d'apprentissage automatique </h4>
                                                                                           
                                                                                            






            </div>

        </body>
        </html>